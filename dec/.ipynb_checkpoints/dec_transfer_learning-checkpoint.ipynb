{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = '../caffe/build/tools:'+os.environ['PATH']\n",
    "import sys\n",
    "sys.path = ['../caffe/python'] + sys.path\n",
    "\n",
    "import cv2\n",
    "import cv\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "import leveldb\n",
    "import caffe\n",
    "from google import protobuf\n",
    "from caffe.proto import caffe_pb2\n",
    "from xml.dom import minidom\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "import cPickle\n",
    "import time\n",
    "\n",
    "class TMM(object):\n",
    "    \"\"\"\n",
    "    TMM class for calculating the p distributions\n",
    "    and finding the cluster centroids\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=1, alpha=1):\n",
    "        self.n_components = n_components\n",
    "        self.tol = 1e-5\n",
    "        self.alpha = float(alpha)\n",
    "\n",
    "    def fit(self, X):\n",
    "        from sklearn.cluster import KMeans\n",
    "        kmeans = KMeans(self.n_components, n_init=20)\n",
    "        kmeans.fit(X)\n",
    "        self.cluster_centers_ = kmeans.cluster_centers_\n",
    "        self.covars_ = np.ones(self.cluster_centers_.shape)\n",
    "\n",
    "    def transform(self, X):\n",
    "        p = 1.0\n",
    "        dist = cdist(X, self.cluster_centers_)\n",
    "        r = 1.0/(1.0+dist**2/self.alpha)**((self.alpha+p)/2.0)\n",
    "        r = (r.T/r.sum(axis=1)).T\n",
    "        return r\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.transform(X).argmax(axis=1)\n",
    "\n",
    "def cluster_acc(Y_pred, Y):\n",
    "    \"\"\"\n",
    "    Finds the cluster accuracy\n",
    "    \"\"\"\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    assert Y_pred.size == Y.size\n",
    "    D = max(Y_pred.max(), Y.max())+1\n",
    "    w = np.zeros((D,D), dtype=np.int64)\n",
    "    for i in xrange(Y_pred.size):\n",
    "        w[Y_pred[i], Y[i]] += 1\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i,j] for i,j in ind])*1.0/Y_pred.size, w\n",
    "\n",
    "def DisKmeans(db, update_interval = None):\n",
    "    \"\"\"\n",
    "    Training pipeline after autoencoding\n",
    "    \"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.mixture import GMM\n",
    "    from sklearn.lda import LDA\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import normalized_mutual_info_score\n",
    "    from scipy.spatial.distance import cdist\n",
    "    import cPickle\n",
    "    from scipy.io import loadmat\n",
    "\n",
    "    if db == 'mnist':\n",
    "        N_class = 10\n",
    "        batch_size = 100\n",
    "        train_batch_size = 256\n",
    "        X, Y = read_db(db+'_total', True)\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        Y = np.asarray(np.squeeze(Y), dtype = np.int32)\n",
    "        N = X.shape[0]\n",
    "        img = np.clip((X/0.02), 0, 255).astype(np.uint8).reshape((N, 28, 28, 1))\n",
    "\n",
    "    tmm_alpha = 1.0\n",
    "    total_iters = (N-1)/train_batch_size+1\n",
    "    if not update_interval:\n",
    "        update_interval = total_iters\n",
    "    Y_pred = np.zeros((Y.shape[0]))\n",
    "    iters = 0\n",
    "    seek = 0\n",
    "    dim = 10\n",
    "\n",
    "    acc_list = []\n",
    "\n",
    "    while True:\n",
    "        print \"Cluster optimization iteration\", iters\n",
    "        write_net(db, dim, N_class, \"'{:08}'\".format(0))\n",
    "        if iters == 0:\n",
    "            write_db(np.zeros((N,N_class)), np.zeros((N,)), 'train_weight')\n",
    "            ret, net = extract_feature('net.prototxt', \n",
    "                        'exp/'+db+'/save_iter_100000.caffemodel', ['output'], N, True, 0)\n",
    "            feature = ret[0].squeeze()\n",
    "\n",
    "            gmm_model = TMM(N_class)\n",
    "            gmm_model.fit(feature)\n",
    "            net.params['loss'][0].data[0,0,:,:] = gmm_model.cluster_centers_.T\n",
    "            net.params['loss'][1].data[0,0,:,:] = 1.0/gmm_model.covars_.T\n",
    "        else:\n",
    "            ret, net = extract_feature('net.prototxt', 'init.caffemodel', ['output'], N, True, 0)\n",
    "            feature = ret[0].squeeze()\n",
    "\n",
    "            gmm_model.cluster_centers_ = net.params['loss'][0].data[0,0,:,:].T\n",
    "\n",
    "\n",
    "        Y_pred_last = Y_pred\n",
    "        Y_pred = gmm_model.predict(feature).squeeze()\n",
    "        acc, freq = cluster_acc(Y_pred, Y)\n",
    "        acc_list.append(acc)\n",
    "        nmi = normalized_mutual_info_score(Y, Y_pred)\n",
    "        print freq\n",
    "        print freq.sum(axis=1)\n",
    "        print 'acc: ', acc, 'nmi: ', nmi\n",
    "        print (Y_pred != Y_pred_last).sum()*1.0/N\n",
    "        if (Y_pred != Y_pred_last).sum() < 0.001*N:\n",
    "            print acc_list\n",
    "            return acc, nmi\n",
    "        time.sleep(1)\n",
    "\n",
    "        write_net(db, dim, N_class, \"'{:08}'\".format(seek))\n",
    "        weight = gmm_model.transform(feature)\n",
    "\n",
    "        weight = (weight.T/weight.sum(axis=1)).T\n",
    "        bias = (1.0/weight.sum(axis=0))\n",
    "        bias = N_class*bias/bias.sum()\n",
    "        weight = (weight**2)*bias\n",
    "        weight = (weight.T/weight.sum(axis=1)).T\n",
    "        print weight[:10,:]\n",
    "        write_db(weight, np.zeros((weight.shape[0],)), 'train_weight')\n",
    "\n",
    "        net.save('init.caffemodel')\n",
    "        del net\n",
    "\n",
    "        with open('solver.prototxt', 'w') as fsolver:\n",
    "            fsolver.write(\"\"\"net: \"net.prototxt\"\n",
    "                base_lr: 0.01\n",
    "                lr_policy: \"step\"\n",
    "                gamma: 0.1\n",
    "                stepsize: 100000\n",
    "                display: 10\n",
    "                max_iter: %d\n",
    "                momentum: 0.9\n",
    "                weight_decay: 0.0000\n",
    "                snapshot: 100\n",
    "                snapshot_prefix: \"exp/test/save\"\n",
    "                snapshot_after_train:true\n",
    "                solver_mode: GPU\n",
    "                debug_info: false\n",
    "                sample_print: false\n",
    "                device_id: 0\"\"\"%update_interval)\n",
    "            \n",
    "        with open(\"reconst_solver.prototxt\", \"w\") as const_solver:\n",
    "            const_solver.write(\"\"\"net: \"pt_net.prototxt\",\n",
    "                base_lr: 0.1\n",
    "                lr_policy: \"step\"\n",
    "                gamma: 0.1\n",
    "                stepsize: 20000\n",
    "                display: 10\n",
    "                test_iter: 100\n",
    "                test_interval: 10000\n",
    "                max_iter: %d\n",
    "                momentum: 0.9\n",
    "                momentum_burnin: 1000\n",
    "                weight_decay: 0.0\n",
    "                snapshot: 10000\n",
    "                snapshot_prefix: \"exp/mnist/save\"\n",
    "                snapshot_after_train:true\n",
    "                solver_mode: GPU\n",
    "                debug_info: false \n",
    "                device_id: 0\n",
    "            \"\"\"%update_interval)\n",
    "        \n",
    "        os.system('caffe train --solver=solver.prototxt --weights=init.caffemodel')\n",
    "        shutil.copyfile('exp/test/save_iter_%d.caffemodel'%update_interval, 'init.caffemodel')\n",
    "\n",
    "        os.system('caffe train --solver=reconst_solver.prototxt --weights=init.caffemodel')\n",
    "        shutil.copyfile('exp/test/save_iter_%d.caffemodel'%update_interval, 'init.caffemodel')\n",
    "                \n",
    "        iters += 1\n",
    "        seek = (seek + train_batch_size*update_interval)%N\n",
    "\n",
    "\"\"\"\n",
    "DB functions\n",
    "\"\"\"\n",
    "def read_db(str_db, float_data = True):\n",
    "    db = leveldb.LevelDB(str_db)\n",
    "    datum = caffe_pb2.Datum()\n",
    "    array = []\n",
    "    label = []\n",
    "    for k,v in db.RangeIter():\n",
    "        dt = datum.FromString(v)\n",
    "        if float_data:\n",
    "            array.append(dt.float_data)\n",
    "        else:\n",
    "            array.append(np.fromstring(dt.data, dtype=np.uint8))\n",
    "        label.append(dt.label)\n",
    "    return np.asarray(array), np.asarray(label)\n",
    "\n",
    "def write_db(X, Y, fname):\n",
    "    if os.path.exists(fname):\n",
    "        shutil.rmtree(fname)\n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "    X = X.reshape((X.shape[0], X.size/X.shape[0], 1, 1))\n",
    "    db = leveldb.LevelDB(fname)\n",
    "\n",
    "    for i in xrange(X.shape[0]):\n",
    "        x = X[i]\n",
    "        if x.ndim != 3:\n",
    "            x = x.reshape((x.size,1,1))\n",
    "        db.Put('{:08}'.format(i), caffe.io.array_to_datum(x, int(Y[i])).SerializeToString())\n",
    "    del db\n",
    "\n",
    "def update_db(seek, N, X, Y, fname):\n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "    X = X.reshape((X.shape[0], X.size/X.shape[0], 1, 1))\n",
    "    db = leveldb.LevelDB(fname)\n",
    "\n",
    "    for i in xrange(X.shape[0]):\n",
    "        x = X[i]\n",
    "        if x.ndim != 3:\n",
    "            x = x.reshape((x.size,1,1))\n",
    "        db.Put('{:08}'.format((i+seek)%N), caffe.io.array_to_datum(x, int(Y[i])).SerializeToString())\n",
    "    del db\n",
    "\n",
    "\"\"\"\n",
    "Caffe network functions\n",
    "\"\"\"\n",
    "def extract_feature(net, model, blobs, N, train = False, device = None):\n",
    "    if type(net) is str:\n",
    "        if train:\n",
    "            caffe.Net.set_phase_train()\n",
    "        if model:\n",
    "            net = caffe.Net(net, model)\n",
    "        else:\n",
    "            net = caffe.Net(net)\n",
    "        caffe.Net.set_phase_test()\n",
    "    if not (device is None):\n",
    "        caffe.Net.set_mode_gpu()\n",
    "        caffe.Net.set_device(device)\n",
    "\n",
    "    batch_size = net.blobs[blobs[0]].num\n",
    "    res = [ [] for i in blobs ]\n",
    "    for i in xrange((N-1)/batch_size+1):\n",
    "        ret = net.forward(blobs=blobs)\n",
    "        for i in xrange(len(blobs)):\n",
    "            res[i].append(ret[blobs[i]].copy())\n",
    "\n",
    "    for i in xrange(len(blobs)):\n",
    "        res[i] = np.concatenate(res[i], axis=0)[:N]\n",
    "\n",
    "    return res, net\n",
    "\n",
    "def write_net(db, dim, n_class, seek):\n",
    "    layers = [ ('data_seek', ('data','dummy',db+'_total', db+'_total', 1.0, seek)),\n",
    "             ('data_seek', ('label', 'dummy', 'train_weight', 'train_weight', 1.0, seek)),\n",
    "\n",
    "             ('inner', ('inner1', 'data', 500)),\n",
    "             ('relu', ('inner1',)),\n",
    "\n",
    "             ('inner', ('inner2', 'inner1', 500)),\n",
    "             ('relu', ('inner2',)),\n",
    "\n",
    "             ('inner', ('inner3', 'inner2', 2000)),\n",
    "             ('relu', ('inner3',)),\n",
    "\n",
    "             ('inner', ('output', 'inner3', dim)),\n",
    "\n",
    "             ('tloss', ('loss', 'output', 'label', n_class))\n",
    "          ]\n",
    "    with open('net.prototxt', 'w') as fnet:\n",
    "        make_net(fnet, layers)\n",
    "\n",
    "\n",
    "def make_net(fnet, layers):\n",
    "    layer_dict = {}\n",
    "    layer_dict['data'] = \"\"\"layers {{\n",
    "        name: \"{0}\"\n",
    "        type: DATA\n",
    "        top: \"{0}\"\n",
    "        data_param {{\n",
    "            source: \"{2}\"\n",
    "            backend: LEVELDB\n",
    "            batch_size: 256\n",
    "        }}\n",
    "        transform_param {{\n",
    "            scale: {4}\n",
    "        }}\n",
    "        include: {{ phase: TRAIN }}\n",
    "    }}\n",
    "    layers {{\n",
    "        name: \"{0}\"\n",
    "        type: DATA\n",
    "        top: \"{0}\"\n",
    "        data_param {{\n",
    "            source: \"{3}\"\n",
    "            backend: LEVELDB\n",
    "            batch_size: 100\n",
    "        }}\n",
    "        transform_param {{\n",
    "            scale: {4}\n",
    "        }}\n",
    "        include: {{ phase: TEST }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    layer_dict['data_seek'] = \"\"\"layers {{\n",
    "        name: \"{0}\"\n",
    "        type: DATA\n",
    "        top: \"{0}\"\n",
    "        data_param {{\n",
    "            seek: {5}\n",
    "            source: \"{2}\"\n",
    "            backend: LEVELDB\n",
    "            batch_size: 256\n",
    "        }}\n",
    "        transform_param {{\n",
    "            scale: {4}\n",
    "        }}\n",
    "        include: {{ phase: TRAIN }}\n",
    "    }}\n",
    "    layers {{\n",
    "        name: \"{0}\"\n",
    "        type: DATA\n",
    "        top: \"{0}\"\n",
    "        data_param {{\n",
    "            seek: {5}\n",
    "            source: \"{3}\"\n",
    "            backend: LEVELDB\n",
    "            batch_size: 100\n",
    "        }}\n",
    "        transform_param {{\n",
    "            scale: {4}\n",
    "        }}\n",
    "        include: {{ phase: TEST }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    layer_dict['sil'] = \"\"\"layers {{\n",
    "      name: \"{0}silence\"\n",
    "      type: SILENCE\n",
    "      bottom: \"{0}\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    layer_dict['tloss'] = \"\"\"layers {{\n",
    "      name: \"{0}\"\n",
    "      type: MULTI_T_LOSS\n",
    "      bottom: \"{1}\"\n",
    "      bottom: \"{2}\"\n",
    "      blobs_lr: 1.\n",
    "      blobs_lr: 0.\n",
    "      blobs_lr: 0.\n",
    "      top: \"loss\"\n",
    "      top: \"std\"\n",
    "      top: \"ind\"\n",
    "      top: \"proba\"\n",
    "      multi_t_loss_param {{\n",
    "        num_center: {3}\n",
    "        alpha: 1\n",
    "        lambda: 2\n",
    "        beta: 1\n",
    "        bandwidth: 0.1\n",
    "        weight_filler {{\n",
    "          type: 'gaussian'\n",
    "          std: 0.5\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "    layers {{\n",
    "      name: \"silence\"\n",
    "      type: SILENCE\n",
    "      bottom: \"label\"\n",
    "      bottom: \"ind\"\n",
    "      bottom: \"proba\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    layer_dict['inner'] = \"\"\"layers {{\n",
    "      name: \"{0}\"\n",
    "      type: INNER_PRODUCT\n",
    "      bottom: \"{1}\"\n",
    "      top: \"{0}\"\n",
    "      blobs_lr: 1\n",
    "      blobs_lr: 2\n",
    "      weight_decay: 1\n",
    "      weight_decay: 0\n",
    "      inner_product_param {{\n",
    "        num_output: {2}\n",
    "        weight_filler {{\n",
    "          type: \"gaussian\"\n",
    "          std: 0.05\n",
    "        }}\n",
    "        bias_filler {{\n",
    "          type: \"constant\"\n",
    "          value: 0\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    layer_dict['inner_init'] = \"\"\"layers {{\n",
    "      name: \"{0}\"\n",
    "      type: INNER_PRODUCT\n",
    "      bottom: \"{1}\"\n",
    "      top: \"{0}\"\n",
    "      blobs_lr: 1\n",
    "      blobs_lr: 2\n",
    "      weight_decay: 1\n",
    "      weight_decay: 0\n",
    "      inner_product_param {{\n",
    "        num_output: {2}\n",
    "        weight_filler {{\n",
    "          type: \"gaussian\"\n",
    "          std: {3}\n",
    "        }}\n",
    "        bias_filler {{\n",
    "          type: \"constant\"\n",
    "          value: 0\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    layer_dict['inner_lr'] = \"\"\"layers {{\n",
    "      name: \"{0}\"\n",
    "      type: INNER_PRODUCT\n",
    "      bottom: \"{1}\"\n",
    "      top: \"{0}\"\n",
    "      blobs_lr: {4}\n",
    "      blobs_lr: {5}\n",
    "      weight_decay: 1\n",
    "      weight_decay: 0\n",
    "      inner_product_param {{\n",
    "        num_output: {2}\n",
    "        weight_filler {{\n",
    "          type: \"gaussian\"\n",
    "          std: {3}\n",
    "        }}\n",
    "        bias_filler {{\n",
    "          type: \"constant\"\n",
    "          value: 0\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    layer_dict['relu'] = \"\"\"layers {{\n",
    "      name: \"{0}relu\"\n",
    "      type: RELU\n",
    "      bottom: \"{0}\"\n",
    "      top: \"{0}\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    layer_dict['drop'] = \"\"\"layers {{\n",
    "      name: \"{0}drop\"\n",
    "      type: DROPOUT\n",
    "      bottom: \"{0}\"\n",
    "      top: \"{0}\"\n",
    "      dropout_param {{\n",
    "        dropout_ratio: {1}\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    layer_dict['drop_copy'] = \"\"\"layers {{\n",
    "      name: \"{0}drop\"\n",
    "      type: DROPOUT\n",
    "      bottom: \"{1}\"\n",
    "      top: \"{0}\"\n",
    "      dropout_param {{\n",
    "        dropout_ratio: {2}\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    layer_dict['euclid'] = \"\"\"layers {{\n",
    "      name: \"{0}\"\n",
    "      type: EUCLIDEAN_LOSS\n",
    "      bottom: \"{1}\"\n",
    "      bottom: \"{2}\"\n",
    "      top: \"{0}\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    fnet.write('name: \"net\"\\n')\n",
    "    for k,v in layers:\n",
    "        fnet.write(layer_dict[k].format(*v))\n",
    "    fnet.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = \"mnist\"\n",
    "N_class = 10\n",
    "X, Y = read_db(db+'_total', True)\n",
    "X = np.asarray(X, dtype=np.float64)\n",
    "Y = np.asarray(np.squeeze(Y), dtype = np.int32)\n",
    "N = X.shape[0]\n",
    "\n",
    "ret, net = extract_feature('net.prototxt', \n",
    "            'exp/'+db+'/save_iter_100000.caffemodel', ['output'], N, True, 0)\n",
    "feature = ret[0].squeeze()\n",
    "\n",
    "gmm_model = TMM(N_class)\n",
    "gmm_model.fit(feature)\n",
    "net.params['loss'][0].data[0,0,:,:] = gmm_model.cluster_centers_.T\n",
    "net.params['loss'][1].data[0,0,:,:] = 1.0/gmm_model.covars_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-33.87748718,  10.34723949,   7.0014267 , -11.88702488,\n",
       "        -5.8514204 , -32.74691772,   5.85734034,  -2.81510282,\n",
       "       -13.64940453,   8.48134041], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_model.cluster_centers_.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 10, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params[\"loss\"][0].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 / gmm_model.covars_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "autoencoder = caffe.Net(\"pt_net.prototxt\", \"exp/mnist/save_iter_100000.caffemodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ret, net = extract_feature(\"pt_net.prototxt\", \"init.caffemodel\",\n",
    "                          [\"pt_loss\"], N, True, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 1, 1, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12338723"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = ret[0].ravel()\n",
    "np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
